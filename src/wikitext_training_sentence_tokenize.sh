python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.1 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.2 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.3 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.4 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.5 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.6 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.7 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.8 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.9 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.10 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.11 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.12 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.13 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.14 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.15 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.16 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.17 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.18 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.19 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.20 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.21 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.22 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.23 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.24 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.25 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.26 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.27 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.28 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.29 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.30 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.31 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.32 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.33 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.34 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.35 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.36 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.37 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.38 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.39 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.40 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.41 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.42 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.43 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.44 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.45 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.46 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.47 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.48 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.49 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.50 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.51 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.52 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.53 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.54 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.55 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.56 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.57 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.58 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.59 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.60 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.61 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.62 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.63 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.64 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.65 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.66 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.67 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.68 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.69 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.70 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.71 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.72 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.73 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.74 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.75 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.76 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.77 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.78 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.79 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.80 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.81 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.82 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.83 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.84 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.85 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.86 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.87 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.88 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.89 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.90 --max_subword_count 400 --unk \<unk\> 
python sentence_tokenize.py --filename ../data/wikitext-103/wiki.train.tokens.detokenized.91 --max_subword_count 400 --unk \<unk\> 
